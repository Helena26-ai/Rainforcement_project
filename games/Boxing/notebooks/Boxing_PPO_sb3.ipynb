{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTICGo-4j68q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecMonitor\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback\n",
        "\n",
        "#Klasa ułatwiająca czytanie logów podczas treningu\n",
        "class EpisodeLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(EpisodeLoggerCallback, self).__init__(verbose)\n",
        "\n",
        "        if self.locals.get(\"infos\") is not None:\n",
        "            for info in self.locals[\"infos\"]:\n",
        "                if info is not None and 'episode' in info:\n",
        "                    # 'r' to nagroda, 'l' to długość epizodu\n",
        "                    episode_reward = info['episode']['r']\n",
        "                    episode_length = info['episode']['l']\n",
        "                    print(f\"Zakończono epizod. Nagroda: {episode_reward:.2f}, Długość: {episode_length}\")\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    env_id = 'BoxingNoFrameskip-v4'\n",
        "    num_envs = 8  # Liczba równoległych środowisk\n",
        "    seed = 42\n",
        "    total_timesteps_finetune = 2_000_000\n",
        "\n",
        "    #Gdzie zapisany jest model jezeli chcemy dotrenowac\n",
        "    pretrained_model_path = 'logs/best_model/best_model.zip'\n",
        "    #Tworzenie katalogow na modele, stany optimizera,tensorboard monitory itd.\n",
        "    finetune_log_dir_base = './logs_finetuned_better'\n",
        "    os.makedirs(finetune_log_dir_base, exist_ok=True)\n",
        "\n",
        "    monitor_csv_path = os.path.join(finetune_log_dir_base, 'monitor_csv_logs')\n",
        "    os.makedirs(monitor_csv_path, exist_ok=True)\n",
        "\n",
        "    finetune_checkpoint_dir = os.path.join(finetune_log_dir_base, 'checkpoints')\n",
        "    os.makedirs(finetune_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    finetune_best_model_dir = os.path.join(finetune_log_dir_base, 'best_model')\n",
        "    os.makedirs(finetune_best_model_dir, exist_ok=True)\n",
        "\n",
        "    finetune_final_model_path = os.path.join(finetune_log_dir_base, 'ppo_boxing_finetuned_final_model.zip')\n",
        "\n",
        "    tensorboard_log_path = os.path.join(finetune_log_dir_base, 'tensorboard_logs')\n",
        "    os.makedirs(tensorboard_log_path, exist_ok=True)\n",
        "\n",
        "    #Każde z równoległych srodowisk ma swoj wlasny monitor\n",
        "    env = make_atari_env(env_id, n_envs=num_envs, seed=seed)\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    env = VecMonitor(env, monitor_csv_path)\n",
        "\n",
        "\n",
        "    # Środowisko ewaluacyjne\n",
        "    eval_env = make_atari_env(env_id, n_envs=1, seed=seed + 100) # Użyj innego seedu dla eval_env\n",
        "    eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "\n",
        "\n",
        "    #Wczytywanie modelu\n",
        "    if not os.path.exists(pretrained_model_path):\n",
        "        print(f\"Nie ma modelu\")\n",
        "        return\n",
        "\n",
        "    print(f\"Model istnieje.\")\n",
        "\n",
        "    custom_hyperparameters = {\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"clip_range\": 0.1,\n",
        "        # \"n_steps\": 2048,\n",
        "        # \"batch_size\": 64,\n",
        "        # \"ent_coef\": 0.01,\n",
        "    }\n",
        "\n",
        "    model = PPO.load(\n",
        "        pretrained_model_path,\n",
        "        env=env\n",
        "        custom_objects={\"learning_rate\": custom_hyperparameters.get(\"learning_rate\"),\n",
        "                        \"clip_range\": custom_hyperparameters.get(\"clip_range\")\n",
        "                       },\n",
        "        tensorboard_log=tensorboard_log_path,\n",
        "        device='auto',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Model wczytany. Liczba kroków przed dostrajaniem: {model.num_timesteps}\")\n",
        "    print(f\"Rozpoczynanie dostrajania. Nowy learning rate: {model.learning_rate}, Clip range: {model.clip_range}\")\n",
        "\n",
        "\n",
        "    checkpoint_save_freq = max(1, 100_000 // num_envs)\n",
        "    checkpoint_callback_finetune = CheckpointCallback(\n",
        "        save_freq=checkpoint_save_freq,\n",
        "        save_path=finetune_checkpoint_dir,\n",
        "        name_prefix='ppo_boxing_finetuned_ckpt'\n",
        "    )\n",
        "\n",
        "    # Ewaluacja i zapis najlepszego modelu\n",
        "    eval_freq = max(1, 25_000 // num_envs)\n",
        "    eval_callback_finetune = EvalCallback(\n",
        "        eval_env,\n",
        "        best_model_save_path=finetune_best_model_dir,\n",
        "        log_path=os.path.join(finetune_log_dir_base, 'eval_logs'),\n",
        "        eval_freq=eval_freq,\n",
        "        n_eval_episodes=5,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    episode_logger_callback = EpisodeLoggerCallback(verbose=1)\n",
        "    print(f\"Rozpoczynanie dostrajania na {total_timesteps_finetune} dodatkowych kroków.\")\n",
        "\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps_finetune,\n",
        "        callback=[checkpoint_callback_finetune, eval_callback_finetune, episode_logger_callback],\n",
        "        reset_num_timesteps=False\n",
        "    )\n",
        "\n",
        "    # Zapis końcowy finalnego modelu po dostrajaniu\n",
        "    model.save(finetune_final_model_path)\n",
        "    print(\"Zakończono proces dostrajania.\")\n"
      ]
    }
  ]
}